{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2072836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# disable gradient\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c68681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen3-1.7B\", device=\"cuda\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08884c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from openai.types.chat import ChatCompletionAssistantMessageParam, ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam\n",
    "\n",
    "Message = ChatCompletionAssistantMessageParam | ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam\n",
    "\n",
    "def get_think_sentences(assistant_content: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given an assistant message content, return all sentences in its <think>...</think> part as a list.\n",
    "    Uses nltk.sent_tokenize for robust sentence splitting.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", assistant_content, re.DOTALL)\n",
    "    if not match:\n",
    "        return []\n",
    "    think_text = match.group(1).strip()\n",
    "    sentences = sent_tokenize(think_text)\n",
    "    return [s for s in sentences if s.strip()]\n",
    "\n",
    "def truncate_last_assistant(messages: list[Message], sentences: list[str], sentence_idx: int):\n",
    "    \"\"\"\n",
    "    Truncate the last assistant message's <think>...</think> content up to the given sentence index.\n",
    "    Splits using the actual sentence string, not tokenization.\n",
    "    \"\"\"\n",
    "    # Find the last assistant message\n",
    "\n",
    "    if messages[-1][\"role\"] != 'assistant':\n",
    "        raise ValueError(\"Last Message is not assistant\")\n",
    "\n",
    "    # Get the content and the <think>...</think> part\n",
    "    content = messages[-1][\"content\"]\n",
    "    think_start = content.find('<think>')\n",
    "    think_end = content.find('</think>')\n",
    "    if think_start == -1 or think_end == -1:\n",
    "        raise ValueError(\"No <think> tags found\")\n",
    "\n",
    "    sentence = sentences[sentence_idx]\n",
    "    new_content = content.split(sentence)[0]\n",
    "    new_messages = messages.copy()\n",
    "    new_messages[-1] = ChatCompletionAssistantMessageParam(role='assistant', content=new_content)\n",
    "    return new_messages\n",
    "\n",
    "def add_probe_phrase(messages: list[Message], mode: Literal[\"MCQ\"]):\n",
    "    if messages[-1][\"role\"] != 'assistant':\n",
    "        raise ValueError(\"Last Message is not assistant\")\n",
    "    \n",
    "    content = messages[-1][\"content\"] + \"</think>\"\n",
    "    \n",
    "    if mode == \"MCQ\":\n",
    "        content = content + \"\\n```json{\\\"answer\\\": \\\"\"\n",
    "\n",
    "    new_messages = messages.copy()\n",
    "    new_messages[-1] = ChatCompletionAssistantMessageParam(role='assistant', content=content)\n",
    "    return new_messages\n",
    "\n",
    "def get_new_messages(history: list[Message], message: Message, sentence_index: int, mode=\"MCQ\"):\n",
    "    think_sentences = get_think_sentences(message[\"content\"])\n",
    "    messages = truncate_last_assistant(\n",
    "        history + [message], think_sentences, sentence_index)\n",
    "    messages = add_probe_phrase(messages, mode=mode)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"tau/commonsense_qa\")[\"validation\"]\n",
    "# can also use allenai/ai2arc as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(question: str, labels: list[str], texts: list[str]) -> list[Message]:\n",
    "\n",
    "    choices = \"\\n\".join([f\"{label}) {text}\" for label,\n",
    "                        text in zip(labels, texts)])\n",
    "    system_message = ChatCompletionSystemMessageParam(role=\"system\", content=\"You are a helpful assistant. For each multiple-choice question, analyze carefully and answer with the correct option only.\")\n",
    "    user_message = ChatCompletionUserMessageParam(role=\"user\", content=f\"Answer the following mcq question {question}\\n{choices}\\nEnsure your final answer is within a json document. Example:\\n```json{{answer: \\\"A\\\"}}```\")\n",
    "    return [system_message, user_message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ed210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history_completion(row_index: int):\n",
    "    history = get_messages(dataset[row_index][\"question\"], dataset[row_index]\n",
    "                            [\"choices\"][\"label\"], dataset[row_index][\"choices\"][\"text\"])\n",
    "    tokens = model.tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(tokens, max_new_tokens=4000, temperature=0.6, return_type=\"tokens\")\n",
    "    # truncate the input tokens from the out\n",
    "    out = out[:, tokens.shape[1]:]\n",
    "    return history, ChatCompletionAssistantMessageParam(role=\"assistant\", content=model.tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583836aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_think_prefixes(history: list[Message], message: Message, mode=\"MCQ\") -> list[list[Message]]:\n",
    "    think_sentences = get_think_sentences(message[\"content\"])\n",
    "    prefixes: list[list[Message]] = []\n",
    "    for i, _ in enumerate(think_sentences):\n",
    "        new_messages = truncate_last_assistant(\n",
    "            history + [message], think_sentences, i)\n",
    "        new_messages = add_probe_phrase(new_messages, mode=mode)\n",
    "        prefixes.append(new_messages)\n",
    "    # with all think sentences\n",
    "    final_message = ChatCompletionAssistantMessageParam(role='assistant', content=message[\"content\"].partition(\"</think>\")[0])\n",
    "    prefixes.append(add_probe_phrase(history + [final_message], mode=mode))\n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair\n",
    "def extract_answer(content: str):\n",
    "    non_thinking = content.split(\"</think>\")[-1].strip()\n",
    "    code_block = non_thinking.partition(\"```\")[-1].strip().partition(\"```\")[0].strip()\n",
    "    result = json_repair.loads(code_block.removeprefix(\"json\").strip())[\"answer\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_reasoning_prefix_top_logprobs(reasoning_prefix: list[Message], choices: list[str]):\n",
    "    choice_tokens = [model.to_single_token(choice) for choice in choices]\n",
    "    tokens = model.tokenizer.apply_chat_template(reasoning_prefix, add_generation_prompt=False, continue_final_message=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "    logits = model(tokens).to(\"cpu\")\n",
    "    return [logits[0, -1, t].item() for t in choice_tokens]\n",
    "\n",
    "def get_logit_evolution(history: list[Message], message: Message, choices: list[str], mode=\"MCQ\"):\n",
    "    prefixes = get_all_think_prefixes(history, message, mode=mode)\n",
    "    return [get_reasoning_prefix_top_logprobs(prefix, choices) for prefix in tqdm(prefixes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def plot_logit_evolution(logits_list, message, choices_labels, choices, question, filename=None):\n",
    "    # logits_list: list of list of 4 floats (timesteps x choices)\n",
    "    # message: the assistant message dict (with 'content')\n",
    "    # choices_labels: list of 4 labels (e.g., ['A', 'B', 'C', 'D'])\n",
    "    # choices: list of 4 choice texts\n",
    "    # question: the question text\n",
    "    # filename: base filename (no extension) to save plot as HTML and PNG in 'plots/'\n",
    "    \n",
    "    # Get answer\n",
    "    answer = extract_answer(message[\"content\"])\n",
    "    \n",
    "    # Get think sentences for hover text and display\n",
    "    think_sentences = get_think_sentences(message[\"content\"])\n",
    "    \n",
    "    # Offset sentences: first point is '', then each sentence\n",
    "    hover_sentences = [''] + think_sentences\n",
    "    \n",
    "    # Prepare data for plotly\n",
    "    x = list(range(len(logits_list)))\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    color_map = ['blue', 'orange', 'green', 'red']\n",
    "    answer_color = 'black'\n",
    "    \n",
    "    for i, (label, choice) in enumerate(zip(choices_labels, choices)):\n",
    "        y = [logits[i] for logits in logits_list]\n",
    "        hover = hover_sentences if len(hover_sentences) == len(logits_list) else ['']*len(logits_list)\n",
    "        is_answer = (label == answer)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x, y=y,\n",
    "            mode='lines+markers',\n",
    "            name=f\"Choice {label}: {choice}\",\n",
    "            line=dict(color=answer_color if is_answer else color_map[i % len(color_map)], width=4 if is_answer else 2),\n",
    "            marker=dict(size=8),\n",
    "            hovertext=hover,\n",
    "            hoverinfo='y'\n",
    "        ))\n",
    "    \n",
    "    html_safe_question = question.replace(\"'\", \"&#39;\").replace('\"', '&quot;').replace(\"\\n\", \"<br>\")\n",
    "    fig.update_layout(\n",
    "        title=f\"Logit Evolution\",\n",
    "        xaxis_title=\"Step (Think Sentence)\",\n",
    "        yaxis_title=\"Logit Value\",\n",
    "        legend_title=\"Choices\",\n",
    "        hovermode=\"x unified\",\n",
    "        margin=dict(t=80, b=80),\n",
    "        # Add the question as a description below the title (for notebook/plotly view)\n",
    "        annotations=[{\n",
    "            'text': f'<b>Question:</b> {html_safe_question}',\n",
    "            'align': 'left',\n",
    "            'showarrow': False,\n",
    "            'xref': 'paper',\n",
    "            'yref': 'paper',\n",
    "            'x': 0,\n",
    "            'y': 1.13,\n",
    "            'bordercolor': 'rgba(0,0,0,0)',\n",
    "            'font': {'size': 14},\n",
    "            'bgcolor': 'rgba(255,255,255,0.95)',\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # Save plot if filename is not None\n",
    "    if filename is not None:\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        html_path = os.path.join('plots', filename + '.html')\n",
    "        png_path = os.path.join('plots', filename + '.png')\n",
    "        js_sentences = json.dumps(hover_sentences)\n",
    "        js_question = json.dumps(f'<div id=\"question-block\" style=\"margin-bottom:18px;padding:10px 12px;background:#f5f5f5;border-left:4px solid #0074D9;font-size:15px;\"><b>Question:</b> {html_safe_question}</div>')\n",
    "        # JS: inject question div above plot, and sentence div below plot\n",
    "        post_script = f'''\n",
    "        var plot = document.getElementsByClassName('plotly-graph-div')[0];\n",
    "        if (plot) {{\n",
    "            var qdiv = document.createElement('div');\n",
    "            qdiv.innerHTML = {js_question};\n",
    "            plot.parentNode.insertBefore(qdiv, plot);\n",
    "            var out = document.createElement('div');\n",
    "            out.id = 'sentence-output';\n",
    "            out.style.margin = '20px 0';\n",
    "            out.style.padding = '10px';\n",
    "            out.style.border = '1px solid #ccc';\n",
    "            out.style.minHeight = '2em';\n",
    "            out.style.fontFamily = 'monospace';\n",
    "            out.style.background = '#f9f9f9';\n",
    "            out.innerHTML = '(No think sentence for this step)';\n",
    "            plot.parentNode.insertBefore(out, plot.nextSibling);\n",
    "            var thinkSentences = {js_sentences};\n",
    "            plot.on('plotly_hover', function(data) {{\n",
    "                var idx = data.points[0].x;\n",
    "                if(idx == 0) {{ out.innerHTML = '(No think sentence for this step)'; }}\n",
    "                else if(idx < thinkSentences.length) {{ out.innerHTML = '<b>Step ' + idx + ':</b> ' + thinkSentences[idx]; }}\n",
    "            }});\n",
    "            plot.on('plotly_unhover', function(data) {{\n",
    "                out.innerHTML = '(No think sentence for this step)';\n",
    "            }});\n",
    "        }}\n",
    "        '''\n",
    "        fig.write_html(html_path, include_plotlyjs='cdn', full_html=True, post_script=post_script)\n",
    "        fig.write_image(png_path)\n",
    "\n",
    "    # Output widget for displaying the sentence (for notebook interactivity)\n",
    "    out = Output()\n",
    "    display(fig)\n",
    "    display(out)\n",
    "\n",
    "    def update_sentence(trace, points, selector):\n",
    "        if points.point_inds:\n",
    "            idx = points.point_inds[0]\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                if idx == 0:\n",
    "                    display(Markdown(\"*(No think sentence for this step)*\"))\n",
    "                elif idx < len(hover_sentences):\n",
    "                    display(Markdown(f\"**Step {idx}:** {hover_sentences[idx]}\"))\n",
    "\n",
    "    # Attach the callback to all traces\n",
    "    for trace in fig.data:\n",
    "        trace.on_hover(update_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    for _ in range(4):\n",
    "        try:\n",
    "            history, message = get_history_completion(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        # exhausted retries\n",
    "        continue\n",
    "    logits = get_logit_evolution(history, message, choices=dataset[i][\"choices\"][\"label\"], mode=\"MCQ\")\n",
    "    choices_labels = dataset[i][\"choices\"][\"label\"]\n",
    "    choices = dataset[i][\"choices\"][\"text\"]\n",
    "    plot_logit_evolution(logits, message, choices_labels, choices, dataset[i][\"question\"], f\"commonsenseqa_{i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
